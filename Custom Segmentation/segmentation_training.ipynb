{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTkC8t5unzxs"
   },
   "source": [
    "First we need to prepare our work environment and install the necessary Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmoGzDJpCXy3"
   },
   "outputs": [],
   "source": [
    "%pip install -q cython==0.29.15 pycocotools # It's needed for installing pycocotools\n",
    "%pip install -q numpy opencv-python-headless \\\n",
    "    torch torchvision \\\n",
    "    albumentations tqdm \\\n",
    "    onnx onnxsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_ujEUJhCXy-"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZDysKMmCXy_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve, urlcleanup\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import albumentations as albm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Flip values for slower training speed, but more determenistic results.\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0L7Ljy_-1M2"
   },
   "source": [
    "# Global variables for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nO6zKVm9-1M3"
   },
   "source": [
    "This training notebook uses COCO dataset: http://cocodataset.org/\n",
    "The annotations in this dataset belong to the COCO Consortium and are licensed under a Creative Commons Attribution 4.0 License. http://cocodataset.org/#termsofuse\n",
    "Images are part of flickr and have corresponding licenses. To check license for each image please refer to the contents of http://images.cocodataset.org/annotations/image_info_unlabeled2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kMzuBP1N-1M3"
   },
   "source": [
    "### Available COCO classes\n",
    "\n",
    "You can check available data here http://cocodataset.org/#explore\n",
    "\n",
    "COCO has 80 categories:\n",
    "\n",
    "person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bbFvx-v-1M4"
   },
   "outputs": [],
   "source": [
    "COCO_CATEGORY = 'pizza'  # target category for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F68rYSMyCXzH"
   },
   "outputs": [],
   "source": [
    "COCO_ANNOTATIONS_PATH = './annotations/instances_train2017.json'\n",
    "COCO_IMAGES_PATH = './train2017/'\n",
    "\n",
    "NUM_TRAINING_STEPS = 4000  # number of steps for training, longer is better\n",
    "VALIDATION_FREQUENCY = 200  # log validation every N steps\n",
    "BATCH_SIZE = 64  # number of images per batch\n",
    "NUM_WORKERS = 2  # number of CPU threads available for image preprocessing\n",
    "\n",
    "# This controls input and output resolution for the network.\n",
    "# Lower values lead to worse mask quality, but faster network inference.\n",
    "# Change carefully.\n",
    "INPUT_HEIGHT = 256   \n",
    "INPUT_WIDTH = 128\n",
    "\n",
    "# This variable below will let this code run on GPU if CUDA device is available.\n",
    "DEVICE = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    torch.cuda.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxt9L7cin9h0"
   },
   "source": [
    "Colab's GPU runtimes don't have enough free drive space to store entire COCO dataset, so we'll have to unpack only the files that we need for training particular category. If you encounter message \"Disk is almost full\" on Google Colab, please click the \"ignore\" button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXBuj4qj-1M8"
   },
   "outputs": [],
   "source": [
    "def download_file(link, filename):\n",
    "    if Path(filename).exists():\n",
    "        return\n",
    "    progress_bar = tqdm(desc=filename,\n",
    "                        dynamic_ncols=True, leave=False,\n",
    "                        mininterval=5, maxinterval=30,\n",
    "                        unit='KiB', unit_scale=True,\n",
    "                        unit_divisor=1024)\n",
    "    def update_progress(count, block_size, total_size):\n",
    "        if progress_bar.total is None:\n",
    "            progress_bar.reset(total_size)\n",
    "        progress_bar.update(count * block_size - progress_bar.n)\n",
    "    urlretrieve(link, filename, reporthook=update_progress)\n",
    "    urlcleanup()\n",
    "    progress_bar.close()\n",
    "\n",
    "\n",
    "if not os.path.isfile('./train2017.zip'):\n",
    "    download_file('http://images.cocodataset.org/zips/train2017.zip', 'train2017.zip')\n",
    "\n",
    "if not os.path.isfile('./annotations_trainval2017.zip'):\n",
    "    download_file('http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
    "                  'annotations_trainval2017.zip')\n",
    "    with ZipFile('./annotations_trainval2017.zip', 'r') as archive:\n",
    "        archive.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JHU605Ag-1M-"
   },
   "source": [
    "Lets load the images and find all image ids that contain our desired class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iz1Vt_2olWcH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coco = COCO(COCO_ANNOTATIONS_PATH)\n",
    "cat_ids = coco.getCatIds(catNms=[COCO_CATEGORY])\n",
    "img_ids = coco.getImgIds(catIds=cat_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AV8sz1KV-1NA"
   },
   "source": [
    "We also need some images that don't contain our desired class.\n",
    "Otherwise the network would always expect to have our object in the input image and will have lots of false positives, like segmenting faces as pizzas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKWii0lslaJZ"
   },
   "outputs": [],
   "source": [
    "with ZipFile('./train2017.zip') as archive:\n",
    "    coco_images_list = archive.namelist()\n",
    "all_ids = [int(i.split('train2017/')[-1][:-4]) for i in coco_images_list[1:]]\n",
    "non_class_img_ids = list(set(all_ids) - set(img_ids))\n",
    "\n",
    "negative_samples = np.random.choice(non_class_img_ids,\n",
    "                                    size=len(img_ids) // 2, \n",
    "                                    replace=False)\n",
    "positive_samples = np.random.choice(img_ids, \n",
    "                                    size=len(img_ids) - (len(img_ids) // 10), \n",
    "                                    replace=False)\n",
    "train_img_ids = np.concatenate((positive_samples, negative_samples))\n",
    "np.random.shuffle(train_img_ids)\n",
    "val_img_ids = np.array(list(set(img_ids) - set(train_img_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GomOL-Zl-1ND"
   },
   "source": [
    "Now we will extract only the images that we need for training, since COCO is pretty large and contains lots of other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USQo89SbtIvz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('./train2017'):\n",
    "    os.mkdir('./train2017')\n",
    "\n",
    "train_images = [str(i).zfill(12) + '.jpg' for i in train_img_ids]\n",
    "val_images = [str(i).zfill(12) + '.jpg' for i in val_img_ids]\n",
    "\n",
    "with ZipFile('./train2017.zip', 'r') as archive:\n",
    "    for image in tqdm(val_images, dynamic_ncols=True, leave=False):\n",
    "        archive.extract('train2017/' + image, './')\n",
    "with ZipFile('./train2017.zip', 'r') as archive:\n",
    "    for image in tqdm(train_images, dynamic_ncols=True, leave=False):\n",
    "        archive.extract('train2017/' + image, './')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJ7LIu_e-1NF"
   },
   "source": [
    "Let's also load an image that we'll use to check the quality of the segmentation while we train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyhFbijHlSVD"
   },
   "outputs": [],
   "source": [
    "TEST_IMG = cv2.imread('./pizza_test_image.jpg')[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkEOGW9XCXzL"
   },
   "outputs": [],
   "source": [
    "plt.imshow(TEST_IMG);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRgISkYMCXzQ"
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oA2E232D-1NJ"
   },
   "source": [
    "This model is based on Mobilenet V2 https://arxiv.org/abs/1801.04381.\n",
    "This is a good starting place for general segmentation model, however there were a range of new architectures optimized for mobile inference, including Mobilenet V3 https://arxiv.org/abs/1905.02244.\n",
    "\n",
    "This model runs on average 12.3ms on iPhone 6S.\n",
    "\n",
    "Our model will use pretrained weights of Mobilenet V2, however these weights assume the input to be RGB in range [0, 1] and input should be normalized. We will disregard this and just use pretrained weights as good initialization for our network. You can play around and check if using the network from scratch without pretrained weights helps you achieve better segmentation quality.\n",
    "\n",
    "### Important point regarding input and output ranges\n",
    "Lens studio feeds the network camera input as RGB images with values in range [0, 255]. This is uncommon in general for neural networks, but in our case we will train the network with this range from scratch, to avoid doing rescaling on network export or import. That's why we need to scale the weights of the first layer – they were trained with normalized values in different range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "01k-Jqn9CXzR"
   },
   "outputs": [],
   "source": [
    "# We also need to replace Mobilenet's ReLU6 activations with ReLU. \n",
    "# There is no noticeable difference in quality, but this will\n",
    "# allow us to use CoreML for mobile inference on iOS devices.\n",
    "def replace_relu6_with_relu(model):\n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        if len(list(module.children())) > 0:\n",
    "            model._modules[name] = replace_relu6_with_relu(model=module)\n",
    "        if isinstance(module, nn.ReLU6):\n",
    "            model._modules[name] = nn.ReLU()\n",
    "    return model\n",
    "\n",
    "\n",
    "class SegmentationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        mobilenet = torchvision.models.mobilenet_v2(width_mult=0.5)\n",
    "\n",
    "        # We reuse state dict from mobilenet v2 width width_mult == 1.0.\n",
    "        # This is not the optimal way to use pretrained models, but in this case\n",
    "        # it gives us good initialization for faster convergence.\n",
    "        state_dict = torchvision.models.mobilenet_v2(pretrained=True).state_dict()\n",
    "        target_dict = mobilenet.state_dict()\n",
    "        for k in target_dict.keys():\n",
    "            if len(target_dict[k].size()) == 0:\n",
    "                continue\n",
    "            state_dict[k] = state_dict[k][:target_dict[k].size(0)]\n",
    "            if len(state_dict[k].size()) > 1:\n",
    "                state_dict[k] = state_dict[k][:, :target_dict[k].size(1)]\n",
    "\n",
    "        mobilenet.load_state_dict(state_dict)\n",
    "\n",
    "        weight = mobilenet.features[0][0].weight.detach()\n",
    "        mobilenet.features[0][0].weight = nn.Parameter(data=weight / 255.)\n",
    "\n",
    "        mobilenet = replace_relu6_with_relu(mobilenet)\n",
    "\n",
    "        self.features = mobilenet.features[:-2]\n",
    "        self.upscale0 = nn.Sequential(\n",
    "            nn.Conv2d(80, 48, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale1 = nn.Sequential(\n",
    "            nn.Conv2d(48, 16, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 8, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale4 = nn.Sequential(\n",
    "            nn.Conv2d(8, 4, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale5 = nn.Conv2d(4, 1, 3, 1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        skip_outs = []\n",
    "        for i in range(len(self.features)):\n",
    "            out = self.features[i](out)\n",
    "            if i in {1, 3, 6, 13}:\n",
    "                skip_outs.append(out)\n",
    "        out = self.upscale0(out)\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale1(out + skip_outs[3])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale2(out + skip_outs[2])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale3(out + skip_outs[1])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale4(out + skip_outs[0])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale5(out)\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PDQk7OQdCXzW"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3nubkgCo-1NM"
   },
   "source": [
    "The class below specifies image loading and augmentations to add variety to our dataset and increase network stability to different input conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dY6I-P5wCXzX"
   },
   "outputs": [],
   "source": [
    "class CocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, coco, image_list,\n",
    "                 category, augmentations):\n",
    "        self.image_folder = image_folder\n",
    "        self.coco = coco\n",
    "        self.image_list = image_list\n",
    "        self.category = category\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image_name = self.image_list[item]\n",
    "        image_path = os.path.join(self.image_folder,\n",
    "                                  str(image_name).zfill(12) + '.jpg')\n",
    "        image = cv2.imread(image_path)[:, :, ::-1]\n",
    "        ann_ids = coco.getAnnIds(imgIds=image_name, catIds=self.category,\n",
    "                                 iscrowd=None)\n",
    "        annotations = coco.loadAnns(ann_ids)\n",
    "        target = []\n",
    "        if len(annotations) == 0:\n",
    "            target = np.zeros((image.shape[0], image.shape[1]))\n",
    "        else:\n",
    "            for ann in annotations:\n",
    "                target.append(coco.annToMask(ann))\n",
    "        target = np.array(target).sum(axis=0)\n",
    "        target = np.clip(target, 0, 1).astype('float')\n",
    "        scaled_width = int(INPUT_HEIGHT * image.shape[1] / image.shape[0])\n",
    "        image = cv2.resize(image, (scaled_width, INPUT_HEIGHT),\n",
    "                           interpolation=cv2.INTER_LINEAR)\n",
    "        target = cv2.resize(target, (scaled_width, INPUT_HEIGHT),\n",
    "                            interpolation=cv2.INTER_NEAREST)\n",
    "        augmented = self.augmentations(image=image, mask=target)\n",
    "        image = augmented['image']\n",
    "        target = augmented['mask']\n",
    "\n",
    "        image = image.astype('float32')\n",
    "\n",
    "        return {'image': image.transpose(2, 0, 1),\n",
    "                'target': target[np.newaxis, :, :].astype('float32')}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqMak-xgCXz9"
   },
   "outputs": [],
   "source": [
    "train_augmentations = albm.Compose([\n",
    "    albm.ShiftScaleRotate(rotate_limit=10),\n",
    "    albm.PadIfNeeded(min_height=INPUT_HEIGHT, min_width=INPUT_WIDTH,\n",
    "                     always_apply=True),\n",
    "    albm.RandomCrop(height=INPUT_HEIGHT, width=INPUT_WIDTH,\n",
    "                    always_apply=True),\n",
    "    albm.HueSaturationValue(),\n",
    "    albm.RandomBrightnessContrast(),\n",
    "    albm.GaussNoise(),\n",
    "    albm.MotionBlur(),\n",
    "    albm.HorizontalFlip()\n",
    "])\n",
    "val_augmentations = albm.Compose([\n",
    "    albm.PadIfNeeded(min_height=INPUT_HEIGHT, min_width=INPUT_WIDTH,\n",
    "                     always_apply=True),\n",
    "    albm.Crop(0, 0, INPUT_WIDTH, INPUT_HEIGHT, always_apply=True)\n",
    "])\n",
    "\n",
    "train_dataset = CocoDataset(COCO_IMAGES_PATH, coco, train_img_ids,\n",
    "                            cat_ids, train_augmentations)\n",
    "val_dataset = CocoDataset(COCO_IMAGES_PATH, coco, val_img_ids, \n",
    "                          cat_ids, val_augmentations)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                           num_workers=NUM_WORKERS, drop_last=True,\n",
    "                                           shuffle=True,\n",
    "                                           worker_init_fn=lambda _: np.random.seed())\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                                         num_workers=NUM_WORKERS, drop_last=False,\n",
    "                                         shuffle=False,\n",
    "                                         worker_init_fn=lambda _: np.random.seed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbhYKOEQCX0C"
   },
   "source": [
    "# Optimizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r99GfMj4-1NQ"
   },
   "source": [
    "Here we set up training and model itself. We'll also set up learning rate scheduler to drop learning rate if our network training platoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9zHT6C_CX0D"
   },
   "outputs": [],
   "source": [
    "model = SegmentationNet()\n",
    "model = model.to(device=DEVICE)\n",
    "loss = nn.BCELoss()\n",
    "loss = loss.to(device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',\n",
    "                                                       factor=.1, patience=3,\n",
    "                                                       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQytrnvX-1NT"
   },
   "source": [
    "For metrics we will only check IoU, accuracy and loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POFl1gYgCX0G"
   },
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, loss_function):\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_value = 0\n",
    "        self.iou = 0\n",
    "        self.accuracy = 0\n",
    "        self.batches = 0\n",
    "\n",
    "    def add_batch(self, prediction, target):\n",
    "        loss_value = self.loss_function(prediction, target)\n",
    "        thresholded_prediction = (prediction >= .5).to(torch.float32)\n",
    "        accuracy = (thresholded_prediction ==\n",
    "                    target).to(torch.float32).mean().item()\n",
    "        intersection = ((thresholded_prediction == 1) & (target == 1)).sum((2, 3))\n",
    "        union = ((thresholded_prediction == 1) | (target == 1)).sum((2, 3))\n",
    "        iou = (intersection.to(torch.float32) /\n",
    "               (union.to(torch.float32) + 1e-7)).mean().item()\n",
    "        self.loss_value = (self.loss_value * self.batches + loss_value) / \\\n",
    "                          (self.batches + 1)\n",
    "        self.iou = (self.iou * self.batches + iou) / (self.batches + 1)\n",
    "        self.accuracy = (self.accuracy * self.batches + accuracy) / \\\n",
    "                        (self.batches + 1)\n",
    "        self.batches += 1\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        self.loss_value = 0\n",
    "        self.iou = 0\n",
    "        self.accuracy = 0\n",
    "        self.batches = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeMd1Qjm-1NV"
   },
   "source": [
    "This helper function will display image and network output side by side to see the progress during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ecTMXFrCX0J"
   },
   "outputs": [],
   "source": [
    "def show_test_image_quality(model, image):\n",
    "    model_input = torch.zeros(1, 3, INPUT_HEIGHT, INPUT_WIDTH,\n",
    "                              dtype=torch.float32, device=DEVICE)\n",
    "    new_height = int(image.shape[0] * INPUT_WIDTH / image.shape[1])\n",
    "    resized_image = cv2.resize(image,\n",
    "                               (INPUT_WIDTH, new_height),\n",
    "                               interpolation=cv2.INTER_CUBIC)\n",
    "    resized_image = resized_image.astype('float32')\n",
    "    model_input[:, :, :new_height, :] = torch.tensor(resized_image.transpose(2, 0, 1))\n",
    "    model_output = model(model_input)\n",
    "    output_mask = model_output[0].detach().cpu().numpy().transpose(1, 2, 0)[:new_height]\n",
    "\n",
    "    output_image = resized_image / 255.\n",
    "    plot_image = np.zeros((output_image.shape[0], output_image.shape[1] * 3, 3))\n",
    "    plot_image[:, :INPUT_WIDTH] = output_image\n",
    "    plot_image[:, INPUT_WIDTH:INPUT_WIDTH * 2] = output_mask\n",
    "    plot_image[:, INPUT_WIDTH * 2:] = output_image * output_mask\n",
    "\n",
    "    plt.imshow(plot_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJ3o5yG_CX0M"
   },
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2W9MNhWsCX0N",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics = Metrics(loss)\n",
    "\n",
    "progress = tqdm(desc='Training progress', total=NUM_TRAINING_STEPS,\n",
    "                dynamic_ncols=True, leave=False)\n",
    "\n",
    "batch_num = 0\n",
    "while batch_num < NUM_TRAINING_STEPS:\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    bn = 0\n",
    "    for batch in train_loader:\n",
    "        batch_num += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch['image'] = batch['image'].to(DEVICE)\n",
    "        batch['target'] = batch['target'].to(DEVICE)\n",
    "\n",
    "        loss_value = loss(model(batch['image']), batch['target'])\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss = (train_loss * bn + loss_value.item()) / (bn + 1)\n",
    "        bn += 1\n",
    "        \n",
    "        # Next two lines make sure we don't update the progress bar too frequently\n",
    "        if batch_num % int(NUM_TRAINING_STEPS / 1000) == 0:\n",
    "            progress.update(int(NUM_TRAINING_STEPS / 1000))\n",
    "\n",
    "        if batch_num % VALIDATION_FREQUENCY == 0:\n",
    "            print(\"[{}] train \\t loss: {:.5f}\".format(batch_num, train_loss))\n",
    "            train_loss = 0\n",
    "            bn = 0\n",
    "\n",
    "            model.eval()\n",
    "            for batch_val in val_loader:\n",
    "                batch_val['image'] = batch_val['image'].to(DEVICE)\n",
    "                batch_val['target'] = batch_val['target'].to(DEVICE)\n",
    "\n",
    "                prediction = model(batch_val['image']).detach()\n",
    "\n",
    "                metrics.add_batch(prediction, batch_val['target'])\n",
    "\n",
    "            print(\"[{}] val \\t loss: {:.5f}\\t IoU: {:.5f}\\t accuracy: {:.5f}\".format(\n",
    "                batch_num, metrics.loss_value, metrics.iou, metrics.accuracy))\n",
    "            scheduler.step(metrics.loss_value)\n",
    "\n",
    "            show_test_image_quality(model, TEST_IMG)\n",
    "            metrics.reset_metrics()\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        if batch_num >= NUM_TRAINING_STEPS:\n",
    "            break\n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ojg_Gei6CX0S"
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36opBBrE-1NY"
   },
   "source": [
    "### Imporant note regarding Lens Studio input and output ranges \n",
    "\n",
    "LensStudio sends RGB image to the network input with values in range [0, 255]. Our network is already trained for that, but if you trained the network with other input range you might need to adjust it when you import your ONNX file.\n",
    "\n",
    "Current network output is in range [0, 1], but again LensStudio needs the output texture to have values in range [0, 255]. That's why we'll use wrapper class for our model that just runs the model and mutiplies output values by 255 to get them to the necessary range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-i4xeXWN-1NZ"
   },
   "outputs": [],
   "source": [
    "class ModelForExport(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMCKsFr0CX0S",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model_for_export = ModelForExport(model)\n",
    "dummy_input = torch.randn(1, 3, INPUT_HEIGHT, INPUT_WIDTH, \n",
    "                          dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "output = model_for_export(dummy_input.detach())\n",
    "\n",
    "input_names = [\"data\"]\n",
    "output_names = [\"prob\"]\n",
    "\n",
    "torch.onnx.export(model_for_export, dummy_input, \n",
    "                  \"./mobilenetv2_pizza_segmentation.onnx\", verbose=False, \n",
    "                  input_names=input_names, output_names=output_names, opset_version=11)",
    "\n",
    "!onnxsim ./mobilenetv2_pizza_segmentation.onnx ./mobilenetv2_pizza_segmentation_sim.onnx"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "segmentation_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
